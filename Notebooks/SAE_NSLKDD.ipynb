{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from BaseOneClass import CentroidBasedOneClassClassifier,Centroid_Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tạo bộ train, test từ các bộ Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder, MaxAbsScaler\n",
    "\n",
    "train_data = np.genfromtxt('Data_fromLoi/NSLKDD_Train.csv', dtype=np.float32, delimiter=',')\n",
    "test_data  = np.genfromtxt('Data_fromLoi/NSLKDD_Test.csv', dtype=np.float32, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal training data:  67343\n",
      "Normal testing data:  9711\n",
      "Anomaly testing data:  12833\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data[:,-1]                #Select label column\n",
    "x_train = train_data[y_train == 0]        #Select only normal data for training  \n",
    "x_train = x_train[:,0:-1]                 #Remove label column\n",
    "print(\"Normal training data: \", x_train.shape[0]) \n",
    "np.random.shuffle(x_train)\n",
    "x_train = x_train[:6734]                  #Sample 5000 connections for training \n",
    "\n",
    "\n",
    "y_test = test_data[:,-1]                  #Select label column  \n",
    "x_test = test_data[:,0:-1]                #Select data except label column\n",
    "\n",
    "test_X0 = x_test[y_test == 0]             #Normal test\n",
    "test_X1 = x_test[y_test > 0]              #Anomaly test \n",
    "print(\"Normal testing data: \", test_X0.shape[0])\n",
    "print(\"Anomaly testing data: \", test_X1.shape[0])\n",
    "\n",
    "x_test = np.concatenate((test_X0, test_X1))\n",
    "\n",
    "test_y0 = np.full((len(test_X0)), True, dtype=bool)\n",
    "test_y1 = np.full((len(test_X1)), False,  dtype=bool)\n",
    "y_test =  np.concatenate((test_y0, test_y1))\n",
    "\n",
    "#create binary label (1-normal, 0-anomaly) for compute AUC later\n",
    "y_test = (y_test).astype(np.int)\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test  = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 500\n",
    "batch_size = 100\n",
    "display_step = 10\n",
    "\n",
    "n_input = x_train.shape[1]\n",
    "print(n_input)\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 85 # 1st layer num features\n",
    "num_hidden_2 = 49 # 2nd layer num features (the latent dim)\n",
    "num_hidden_3 = 12 # 3nd layer num features (the latent dim)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute The Area Under ROC Curve\n",
    "def AUC_AE(x_test, y_test):\n",
    "    recon_X      = sess.run(decoder_op, feed_dict={X:x_test})\n",
    "    recon_errors = ((recon_X - x_test)**2).mean(1)\n",
    "    \n",
    "    predictions = -recon_errors\n",
    "    FPR, TPR, thresholds = roc_curve(y_test, predictions)\n",
    "    auc_ae = auc(FPR, TPR)\n",
    "    return FPR, TPR, auc_ae\n",
    "\n",
    "#Function to compute The Area Under ROC Curve\n",
    "def AUC_SVM(z_train, z_test, y_test):\n",
    "    #- Trainning SVM using Z\n",
    "    clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "    clf.fit(z_train)\n",
    "    z_pred_test = clf.decision_function(z_test)\n",
    "    \n",
    "    predictions = z_pred_test\n",
    "    FPR, TPR, thresholds = roc_curve(y_test, predictions)\n",
    "    auc_svm = auc(FPR, TPR)\n",
    "    return FPR, TPR, auc_svm\n",
    "\n",
    "def AUC_CEN(z_train, z_test, y_test):\n",
    "    CEN = CentroidBasedOneClassClassifier()\n",
    "    CEN.fit(z_train)  \n",
    "    z_pred_test = -CEN.get_density(z_test)\n",
    "    \n",
    "    FPR, TPR, thresholds = roc_curve(y_test, z_pred_test)\n",
    "    auc_cen = auc(FPR, TPR) \n",
    "    return FPR, TPR, auc_cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    MaxAbs_data = tf.reduce_max(tf.abs(data), axis =0)\n",
    "    #m = sess.run(MaxAbs_data)\n",
    "    data_norm =   data/MaxAbs_data\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "weights_en = {\n",
    "    'encoder_h1': tf.Variable(xavier_init([n_input, num_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(xavier_init([num_hidden_1, num_hidden_2])),\n",
    "    'encoder_h3': tf.Variable(xavier_init([num_hidden_2, num_hidden_3]))\n",
    "}\n",
    "\n",
    "weights_de = {\n",
    "    'decoder_h1': tf.transpose(weights_en['encoder_h3']),    #12-49\n",
    "    'decoder_h2': tf.transpose(weights_en['encoder_h2']),    #49 - 85\n",
    "    'decoder_h3': tf.transpose(weights_en['encoder_h1']),    #85 - 122 \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.zeros(shape=[num_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.zeros(shape=[num_hidden_2])),  \n",
    "    'encoder_b3': tf.Variable(tf.zeros(shape=[num_hidden_3])), \n",
    "    \n",
    "    'decoder_b1': tf.Variable(tf.zeros(shape=[num_hidden_2])), \n",
    "    'decoder_b2': tf.Variable(tf.zeros(shape=[num_hidden_1])),\n",
    "    'decoder_b3': tf.Variable(tf.zeros(shape=[n_input]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation\n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights_en['encoder_h1']), biases['encoder_b1']))\n",
    "    \n",
    "    # Encoder Hidden layer with sigmoid activation\n",
    "    layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights_en['encoder_h2']), biases['encoder_b2']))\n",
    "\n",
    "    # Encoder Hidden layer with sigmoid activation\n",
    "    layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights_en['encoder_h3']), biases['encoder_b3']))\n",
    "    \n",
    "    return layer_3\n",
    "\n",
    "\n",
    "\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation \n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights_de['decoder_h1']), biases['decoder_b1']))\n",
    "    \n",
    "    # Decoder Hidden layer with sigmoid activation\n",
    "    layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights_de['decoder_h2']), biases['decoder_b2']))\n",
    "\n",
    "    layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights_de['decoder_h3']), biases['decoder_b3']))\n",
    "    \n",
    "    return layer_3\n",
    "\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "alpha=10\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "x_encoder=encoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "#x_encoder1 = normalize_data(x_encoder)\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))+ alpha*tf.reduce_mean(tf.pow(x_encoder,2))\n",
    "#loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))+ tf.reduce_mean(tf.pow(x_encoder1,2))\n",
    "\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step   0: Minibatch Loss: 1.2973 - AUC_AE 0.755 - AUC_SVM:0.894 - AUC_CEN:0.817\n",
      "Step   1: Minibatch Loss: 1.1992 - AUC_AE 0.780 - AUC_SVM:0.896 - AUC_CEN:0.828\n",
      "Step  10: Minibatch Loss: 0.5800 - AUC_AE 0.873 - AUC_SVM:0.908 - AUC_CEN:0.903\n",
      "Step  20: Minibatch Loss: 0.3534 - AUC_AE 0.911 - AUC_SVM:0.919 - AUC_CEN:0.923\n",
      "Step  30: Minibatch Loss: 0.2503 - AUC_AE 0.932 - AUC_SVM:0.924 - AUC_CEN:0.927\n",
      "Step  40: Minibatch Loss: 0.1897 - AUC_AE 0.938 - AUC_SVM:0.930 - AUC_CEN:0.927\n",
      "Step  50: Minibatch Loss: 0.1505 - AUC_AE 0.941 - AUC_SVM:0.932 - AUC_CEN:0.928\n",
      "Step  60: Minibatch Loss: 0.1238 - AUC_AE 0.943 - AUC_SVM:0.934 - AUC_CEN:0.930\n",
      "Step  70: Minibatch Loss: 0.1045 - AUC_AE 0.948 - AUC_SVM:0.936 - AUC_CEN:0.934\n",
      "Step  80: Minibatch Loss: 0.0900 - AUC_AE 0.948 - AUC_SVM:0.939 - AUC_CEN:0.938\n",
      "Step  90: Minibatch Loss: 0.0787 - AUC_AE 0.946 - AUC_SVM:0.942 - AUC_CEN:0.942\n",
      "Step 100: Minibatch Loss: 0.0697 - AUC_AE 0.944 - AUC_SVM:0.943 - AUC_CEN:0.945\n",
      "Step 110: Minibatch Loss: 0.0623 - AUC_AE 0.943 - AUC_SVM:0.945 - AUC_CEN:0.948\n",
      "Step 120: Minibatch Loss: 0.0562 - AUC_AE 0.942 - AUC_SVM:0.949 - AUC_CEN:0.953\n",
      "Step 130: Minibatch Loss: 0.0512 - AUC_AE 0.941 - AUC_SVM:0.953 - AUC_CEN:0.957\n",
      "Step 140: Minibatch Loss: 0.0470 - AUC_AE 0.939 - AUC_SVM:0.954 - AUC_CEN:0.958\n",
      "Step 150: Minibatch Loss: 0.0435 - AUC_AE 0.937 - AUC_SVM:0.956 - AUC_CEN:0.960\n",
      "Step 160: Minibatch Loss: 0.0406 - AUC_AE 0.935 - AUC_SVM:0.956 - AUC_CEN:0.960\n",
      "Step 170: Minibatch Loss: 0.0381 - AUC_AE 0.934 - AUC_SVM:0.957 - AUC_CEN:0.961\n",
      "Step 180: Minibatch Loss: 0.0359 - AUC_AE 0.933 - AUC_SVM:0.957 - AUC_CEN:0.961\n",
      "Step 190: Minibatch Loss: 0.0341 - AUC_AE 0.932 - AUC_SVM:0.957 - AUC_CEN:0.961\n",
      "Step 200: Minibatch Loss: 0.0326 - AUC_AE 0.931 - AUC_SVM:0.958 - AUC_CEN:0.961\n",
      "Step 210: Minibatch Loss: 0.0312 - AUC_AE 0.930 - AUC_SVM:0.959 - AUC_CEN:0.962\n",
      "Step 220: Minibatch Loss: 0.0301 - AUC_AE 0.929 - AUC_SVM:0.959 - AUC_CEN:0.962\n",
      "Step 230: Minibatch Loss: 0.0291 - AUC_AE 0.928 - AUC_SVM:0.959 - AUC_CEN:0.962\n",
      "Step 240: Minibatch Loss: 0.0282 - AUC_AE 0.928 - AUC_SVM:0.960 - AUC_CEN:0.962\n",
      "Step 250: Minibatch Loss: 0.0274 - AUC_AE 0.927 - AUC_SVM:0.960 - AUC_CEN:0.962\n",
      "Step 260: Minibatch Loss: 0.0268 - AUC_AE 0.927 - AUC_SVM:0.960 - AUC_CEN:0.962\n",
      "Step 270: Minibatch Loss: 0.0262 - AUC_AE 0.927 - AUC_SVM:0.960 - AUC_CEN:0.962\n",
      "Step 280: Minibatch Loss: 0.0256 - AUC_AE 0.926 - AUC_SVM:0.960 - AUC_CEN:0.962\n",
      "Step 290: Minibatch Loss: 0.0252 - AUC_AE 0.926 - AUC_SVM:0.959 - AUC_CEN:0.961\n",
      "Step 300: Minibatch Loss: 0.0247 - AUC_AE 0.925 - AUC_SVM:0.959 - AUC_CEN:0.961\n",
      "Step 310: Minibatch Loss: 0.0244 - AUC_AE 0.925 - AUC_SVM:0.959 - AUC_CEN:0.961\n",
      "Step 320: Minibatch Loss: 0.0240 - AUC_AE 0.925 - AUC_SVM:0.959 - AUC_CEN:0.960\n",
      "Step 330: Minibatch Loss: 0.0237 - AUC_AE 0.925 - AUC_SVM:0.959 - AUC_CEN:0.960\n",
      "Step 340: Minibatch Loss: 0.0234 - AUC_AE 0.924 - AUC_SVM:0.959 - AUC_CEN:0.960\n",
      "Step 350: Minibatch Loss: 0.0231 - AUC_AE 0.924 - AUC_SVM:0.959 - AUC_CEN:0.960\n",
      "Step 360: Minibatch Loss: 0.0229 - AUC_AE 0.924 - AUC_SVM:0.958 - AUC_CEN:0.960\n",
      "Step 370: Minibatch Loss: 0.0227 - AUC_AE 0.924 - AUC_SVM:0.958 - AUC_CEN:0.960\n",
      "Step 380: Minibatch Loss: 0.0225 - AUC_AE 0.924 - AUC_SVM:0.958 - AUC_CEN:0.959\n",
      "Step 390: Minibatch Loss: 0.0223 - AUC_AE 0.924 - AUC_SVM:0.958 - AUC_CEN:0.959\n",
      "Step 400: Minibatch Loss: 0.0221 - AUC_AE 0.924 - AUC_SVM:0.958 - AUC_CEN:0.959\n",
      "Step 410: Minibatch Loss: 0.0219 - AUC_AE 0.923 - AUC_SVM:0.958 - AUC_CEN:0.959\n",
      "Step 420: Minibatch Loss: 0.0218 - AUC_AE 0.923 - AUC_SVM:0.958 - AUC_CEN:0.959\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# Pharse 1: Train for Auto Encoder Model\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "num_batch = int(x_train.shape[0]/batch_size)\n",
    "# Training\n",
    "for i in range(num_steps):\n",
    "    # Prepare Data\n",
    "    re = 0\n",
    "    for i_batch in range(num_batch):\n",
    "        batch_x = x_train[i_batch*batch_size:(i_batch+1)*batch_size] \n",
    "        _, re_batch = sess.run([optimizer, loss], feed_dict={X: batch_x})\n",
    "        re = re + re_batch\n",
    "        # Display logs per step\n",
    "    if i % display_step == 0 or i == 1:\n",
    "        z_train = sess.run(x_encoder,feed_dict={X:x_train})\n",
    "        z_test = sess.run(x_encoder,feed_dict={X:x_test})\n",
    "        \n",
    "        _,_,auc_ae    = AUC_AE(x_test, y_test)\n",
    "        _,_,auc_svm = AUC_SVM(z_train, z_test, y_test)\n",
    "        _,_,auc_cen = AUC_CEN(z_train, z_test, y_test)\n",
    "        print('Step %3.0i: Minibatch Loss: %0.4f - AUC_AE %0.3f - AUC_SVM:%0.3f - AUC_CEN:%0.3f' % (i, re/num_batch, auc_ae, auc_svm, auc_cen ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
